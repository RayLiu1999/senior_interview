# 如何設計分散式鍵值儲存系統？

- **難度**: 9
- **重要程度**: 5
- **標籤**: `System Design`, `Key-Value Store`, `Distributed`, `Consistency`

## 問題詳述

分散式鍵值儲存系統（如 Redis、DynamoDB、etcd）是現代分散式系統的基礎組件。請設計一個高可用、可擴展的分散式 KV 儲存系統，需要考慮：

1. **核心功能**：如何實現 GET、PUT、DELETE 等基本操作？
2. **資料分片**：如何將資料分佈到多個節點？
3. **副本策略**：如何保證資料可靠性和高可用性？
4. **一致性模型**：如何在 CAP 定理下做出權衡？
5. **故障處理**：如何檢測和恢復節點故障？

## 核心理論與詳解

分散式鍵值儲存是一個經典的分散式系統設計問題，涉及分片、副本、一致性、故障恢復等核心概念。

### 1. 核心功能設計

#### 基本 API

- **PUT(key, value)**：寫入或更新鍵值對
- **GET(key)**：讀取指定鍵的值
- **DELETE(key)**：刪除指定鍵
- **EXISTS(key)**：檢查鍵是否存在
- **SCAN(prefix)**：掃描特定前綴的鍵（可選）

#### 資料結構

- **鍵（Key）**：
  - 通常為字串類型
  - 有長度限制（如最大 256 字節）
  - 需要支援高效的雜湊運算

- **值（Value）**：
  - 可以是任意二進制數據
  - 有大小限制（如最大 1MB）
  - 可選擇是否壓縮儲存

- **元數據**：
  - 版本號（version）：用於樂觀鎖和衝突檢測
  - 時間戳（timestamp）：記錄最後修改時間
  - TTL（Time to Live）：過期時間

### 2. 資料分片策略

當資料量超過單機容量時，需要將資料分散到多個節點。

#### 雜湊分片

- **基本雜湊**：
  - 計算 `hash(key) % N`，N 為節點數
  - 優點：實現簡單，分佈均勻
  - 缺點：節點數變化時需要重新雜湊大量資料

#### 一致性雜湊（Consistent Hashing）

- **原理**：
  - 將雜湊空間視為一個環（0 到 2^32-1）
  - 節點和鍵都映射到環上
  - 鍵順時針查找最近的節點作為負責節點

- **優勢**：
  - 節點增減時，只影響相鄰節點的資料
  - 平均只需遷移 K/N 的資料（K 為總鍵數，N 為節點數）

- **虛擬節點**：
  - 每個物理節點映射多個虛擬節點（如 150 個）
  - 解決資料分佈不均的問題
  - 新節點加入時從多個節點分擔負載

#### 範圍分片（Range Partitioning）

- **原理**：
  - 將鍵空間劃分為多個連續範圍
  - 例如：[a-f] → Node1, [g-m] → Node2, [n-z] → Node3

- **優勢**：
  - 支援範圍查詢（如 SCAN 操作）
  - 便於順序遍歷

- **劣勢**：
  - 可能出現熱點問題（某個範圍的鍵訪問頻繁）
  - 需要動態分裂和合併範圍

### 3. 副本與複製策略

為了保證資料可靠性和高可用性，每份資料通常儲存多個副本。

#### 副本數量

- **典型配置**：3 個副本
- **權衡**：
  - 副本越多，可靠性越高，但儲存成本越高
  - 寫入延遲也會增加（需要同步到多個副本）

#### 副本放置策略

- **機架感知**：
  - 將副本放置在不同機架或機房
  - 避免單點故障（機架斷電、交換機故障）

- **地理分佈**：
  - 跨地域部署副本
  - 提供災難恢復能力
  - 但會增加網路延遲

#### 主從複製（Primary-Backup）

- **架構**：
  - 每個分片有一個主副本（Primary）和多個從副本（Backup）
  - 所有寫入先到主副本，然後同步到從副本
  - 讀取可以從主副本或從副本進行

- **同步策略**：
  - **同步複製**：寫入必須同步到所有副本才算成功
    - 保證強一致性
    - 寫入延遲高
  - **非同步複製**：寫入主副本後立即返回
    - 寫入延遲低
    - 可能丟失最新資料（主副本故障時）
  - **半同步複製**：寫入主副本和至少一個從副本
    - 平衡一致性和性能

### 4. 一致性模型

CAP 定理指出，分散式系統無法同時滿足一致性（Consistency）、可用性（Availability）和分區容錯性（Partition Tolerance），最多只能滿足兩項。

#### 強一致性（Strong Consistency）

- **保證**：任何讀操作都能讀到最新寫入的值
- **實現**：
  - 使用 Paxos 或 Raft 共識演算法
  - 所有寫入必須獲得多數節點的確認
- **範例**：etcd、ZooKeeper
- **代價**：寫入延遲高，可用性降低

#### 最終一致性（Eventual Consistency）

- **保證**：如果沒有新的更新，最終所有副本會收斂到相同狀態
- **實現**：
  - 非同步複製
  - 使用向量時鐘（Vector Clock）或版本號追蹤因果關係
- **範例**：DynamoDB、Cassandra
- **優勢**：高可用性，低延遲
- **劣勢**：可能讀到舊資料

#### Quorum 機制

- **讀寫策略**：
  - N：副本總數
  - W：寫入時需要確認的副本數
  - R：讀取時需要查詢的副本數
  - 當 W + R > N 時，保證讀到最新資料

- **典型配置**：
  - N=3, W=2, R=2：平衡一致性和可用性
  - N=3, W=1, R=3：優化寫入性能
  - N=3, W=3, R=1：優化讀取性能

#### 衝突解決

當使用最終一致性時，可能出現寫衝突。

- **Last-Write-Wins (LWW)**：
  - 使用時間戳，最新的寫入獲勝
  - 簡單但可能丟失資料

- **Vector Clocks**：
  - 追蹤每個副本的版本向量
  - 識別因果關係和並發寫入
  - 保留衝突版本，交給應用層解決

- **CRDT（Conflict-free Replicated Data Types）**：
  - 使用特殊的資料結構（如計數器、集合）
  - 保證合併操作的交換律和結合律
  - 自動解決衝突

### 5. 故障檢測與恢復

#### 心跳機制

- **實現**：
  - 節點定期向協調器（或鄰居節點）發送心跳
  - 超時未收到心跳則標記為故障

- **Gossip 協議**：
  - 節點之間隨機交換狀態資訊
  - 故障資訊會在集群中快速傳播
  - 去中心化，無單點故障

#### 主副本故障切換

- **故障檢測**：
  - 從副本檢測到主副本心跳超時
  - 觸發選舉流程

- **選舉算法**：
  - **Raft**：
    - 候選者向其他節點請求投票
    - 獲得多數票的候選者成為新主副本
    - 保證同一任期內只有一個主副本
  - **Bully 算法**：
    - 最高 ID 的存活節點成為主副本
    - 較簡單但不夠健壯

- **資料恢復**：
  - 新主副本從日誌中恢復狀態
  - 從副本從新主副本同步最新資料

#### 腦裂問題

當網路分區導致集群分裂成多個子集群時，可能出現多個主副本。

- **解決方案**：
  - **多數派機制**：只有包含多數節點的分區可以繼續服務
  - **Fencing**：舊主副本被隔離，無法寫入共享資源
  - **Generation/Epoch Number**：使用遞增的世代號，舊世代的主副本被拒絕

### 6. 資料持久化

#### 記憶體 + 磁碟混合

- **Write-Ahead Log (WAL)**：
  - 所有寫入先追加到日誌文件
  - 日誌是順序寫入，速度快
  - 保證資料持久性

- **記憶體表（MemTable）**：
  - 最新資料在記憶體中維護
  - 使用跳錶或紅黑樹等資料結構
  - 支援高效的讀寫

- **SSTable（Sorted String Table）**：
  - MemTable 達到閾值後刷寫到磁碟
  - 資料按鍵排序，便於二分查找
  - 不可變，寫入後不再修改

#### LSM Tree（Log-Structured Merge Tree）

- **多層結構**：
  - Level 0：最新的 SSTable（可能有重疊）
  - Level 1-N：更老的 SSTable（無重疊，已合併）

- **合併（Compaction）**：
  - 定期將多個 SSTable 合併成一個
  - 清除已刪除的鍵和舊版本
  - 減少檔案數量，提升讀取效能

- **優勢**：
  - 寫入性能極高（順序寫入）
  - 適合寫入密集型場景

- **劣勢**：
  - 讀取可能需要查詢多個 SSTable
  - 需要使用 Bloom Filter 加速查詢

### 7. 性能優化

#### Bloom Filter

- **用途**：快速判斷鍵是否可能存在
- **原理**：
  - 使用多個雜湊函數將鍵映射到位元陣列
  - 查詢時檢查對應位元是否都為 1
- **特性**：
  - 可能有誤判（不存在的鍵判斷為存在）
  - 不會有漏判（存在的鍵一定判斷為存在）
- **效果**：大幅減少無效的磁碟讀取

#### 快取策略

- **熱鍵快取**：
  - 將頻繁訪問的鍵值對快取在記憶體
  - 使用 LRU 或 LFU 淘汰策略

- **查詢快取**：
  - 快取最近的查詢結果
  - 減少重複查詢的開銷

#### 壓縮

- **鍵壓縮**：
  - 使用前綴壓縮（相同前綴只儲存一次）
  - 減少儲存空間

- **值壓縮**：
  - 使用 Snappy、LZ4 等快速壓縮算法
  - 平衡壓縮率和 CPU 開銷

### 8. 高級功能

#### 事務支援

- **單鍵事務**：
  - 使用 CAS（Compare-And-Swap）
  - 檢查版本號，只有版本匹配時才更新

- **多鍵事務**：
  - 使用兩階段提交（2PC）
  - 協調者確保所有參與者都準備好後才提交
  - 但可能阻塞（協調者故障時）

- **分散式事務**：
  - Percolator 模型（Google Bigtable）
  - Spanner 使用 TrueTime API 保證外部一致性

#### 範圍查詢

- **實現方式**：
  - 使用範圍分片
  - 或在每個節點上維護 B+ Tree 索引

- **挑戰**：
  - 可能跨越多個分片
  - 需要聚合多個節點的結果

#### TTL 與過期

- **被動過期**：
  - 讀取時檢查是否過期
  - 過期則刪除並返回不存在

- **主動過期**：
  - 後台任務定期掃描並刪除過期鍵
  - 使用時間輪（Timing Wheel）等高效資料結構

### 9. 監控與運維

#### 關鍵指標

- **性能指標**：
  - QPS（每秒查詢數）
  - P99 延遲（99% 請求的延遲）
  - 命中率（快取命中率）

- **資源指標**：
  - CPU 使用率
  - 記憶體使用率
  - 磁碟 I/O
  - 網路頻寬

- **可靠性指標**：
  - 副本同步延遲
  - 故障切換時間
  - 資料丟失率

#### 運維操作

- **擴容**：
  - 新增節點並重新平衡資料
  - 使用一致性雜湊減少資料遷移量

- **備份與恢復**：
  - 定期快照資料
  - 支援 Point-in-Time Recovery

- **跨地域複製**：
  - 非同步複製到其他地域
  - 提供災難恢復能力

### 10. 真實系統案例

#### Redis

- **特點**：單機、記憶體、高性能
- **持久化**：RDB 快照 + AOF 日誌
- **高可用**：Sentinel 哨兵模式、Cluster 集群模式
- **應用**：快取、會話儲存、排行榜

#### DynamoDB

- **特點**：全託管、最終一致性、無限擴展
- **分片**：一致性雜湊
- **一致性**：支援最終一致性和強一致性讀
- **應用**：大規模 Web 應用、物聯網

#### etcd

- **特點**：強一致性、基於 Raft
- **應用**：配置管理、服務發現、分散式鎖
- **保證**：Linearizable 讀寫

#### Cassandra

- **特點**：去中心化、最終一致性、高可用
- **分片**：一致性雜湊
- **複製**：可配置的複製因子和一致性級別
- **應用**：時間序列資料、日誌儲存

## 架構總結

一個完整的分散式鍵值儲存系統包含：

1. **客戶端層**：SDK、連接池、重試機制
2. **協調層**：元數據管理、路由表、故障檢測
3. **儲存層**：資料分片、副本管理、持久化
4. **共識層**：Raft/Paxos（強一致性系統）
5. **監控層**：性能監控、告警、日誌

設計分散式 KV 儲存需要在一致性、可用性、性能、成本之間做出權衡，沒有銀彈方案，需要根據具體業務需求選擇合適的設計。
