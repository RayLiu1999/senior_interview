# 如何設計分散式鎖？

- **難度**: 8
- **重要程度**: 5
- **標籤**: `System Design`, `Distributed Lock`, `Redis`, `ZooKeeper`, `Consensus`

## 問題詳述

設計一個分散式鎖系統，用於分散式環境下的資源協調和互斥訪問。系統需要保證互斥性、避免死鎖、支援鎖超時和續期，並處理網路分區等異常情況。

## 核心理論與詳解

### 1. 為什麼需要分散式鎖

#### 1.1 單機鎖的局限性

**單機環境**：
```
場景：單個進程內的多線程
解決方案：synchronized、mutex、semaphore
效果：✅ 完美解決

場景：同一台機器的多進程
解決方案：文件鎖、信號量
效果：✅ 可以解決
```

**分散式環境**：
```
場景：多台服務器、多個進程
問題：單機鎖無法跨機器生效
需要：分散式協調機制
```

#### 1.2 典型應用場景

**場景一：秒殺庫存扣減**
```
問題：
- 10 台服務器同時處理秒殺請求
- 需要保證庫存不超賣
- 單機鎖無效（每台服務器各自加鎖）

解決：
- 使用分散式鎖
- 同一時刻只有一台服務器能扣減庫存
```

**場景二：定時任務防重複執行**
```
問題：
- 3 台服務器都部署了相同的定時任務
- 任務只需執行一次
- 如何避免重複執行？

解決：
- 任務開始前嘗試獲取分散式鎖
- 只有獲取成功的服務器執行任務
```

**場景三：分散式事務**
```
問題：
- 訂單服務和庫存服務需要協調操作
- 避免並發導致的數據不一致

解決：
- 使用分散式鎖協調操作順序
```

### 2. 分散式鎖的核心要求

一個合格的分散式鎖必須滿足以下要求：

#### 2.1 必須滿足的要求

**互斥性（Mutual Exclusion）**：
```
定義：同一時刻只有一個客戶端能持有鎖

測試方法：
- 多個客戶端同時嘗試獲取鎖
- 只有一個能成功
- 其他的必須等待或失敗
```

**避免死鎖（Deadlock Free）**：
```
定義：即使持有鎖的客戶端崩潰，鎖也能被釋放

實現方式：
- 鎖設置過期時間（TTL）
- 超時自動釋放
```

**容錯性（Fault Tolerance）**：
```
定義：部分節點故障不影響鎖服務

實現方式：
- 使用集群部署
- 主從複製或多副本
```

#### 2.2 進階要求

**解鈴還須繫鈴人（Ownership）**：
```
定義：只有加鎖的客戶端才能解鎖

問題場景：
- 客戶端 A 獲取鎖
- 鎖過期自動釋放
- 客戶端 B 獲取鎖
- 客戶端 A 完成任務，嘗試解鎖
- ❌ 客戶端 A 不應該能解除 B 的鎖

解決方案：
- 鎖帶有唯一標識（如 UUID）
- 解鎖時驗證標識
```

**可重入性（Reentrant）**：
```
定義：同一客戶端可以多次獲取同一把鎖

應用場景：
- 遞迴函數需要多次加鎖
- 方法 A 調用方法 B，都需要同一把鎖
```

### 3. 實現方案對比與選擇

#### 3.1 方案一：數據庫實現

**實現方式**：
```sql
-- 創建鎖表
CREATE TABLE distributed_lock (
    lock_name VARCHAR(64) PRIMARY KEY,
    owner VARCHAR(64),
    expiration TIMESTAMP
);

-- 獲取鎖（利用主鍵唯一性）
INSERT INTO distributed_lock (lock_name, owner, expiration) 
VALUES ('resource_123', 'client_a', NOW() + INTERVAL 10 SECOND);

-- 釋放鎖
DELETE FROM distributed_lock 
WHERE lock_name = 'resource_123' AND owner = 'client_a';
```

**優缺點分析**：
- ✅ 實現簡單，大部分系統都有數據庫
- ✅ 強一致性（ACID 保證）
- ❌ **性能差**：數據庫 QPS 有限（單機 ~1000）
- ❌ **單點故障**：數據庫故障導致鎖服務不可用
- ❌ 需要定時清理過期鎖（額外任務）

**結論**：僅適合低並發、對性能要求不高的場景。

#### 3.2 方案二：Redis 實現（推薦）

**為什麼選擇 Redis**：
- ✅ **性能極高**：單機 10 萬+ QPS
- ✅ **原子操作**：支持原子性的 SET NX EX 命令
- ✅ **過期自動刪除**：TTL 機制天然支持
- ✅ **部署廣泛**：大部分系統已有 Redis

**基礎實現**：
```
1. 獲取鎖：
   SET lock_key unique_value NX EX 10
   
   - NX：僅在 key 不存在時設置
   - EX 10：10 秒後自動過期
   - unique_value：唯一標識（UUID）

2. 釋放鎖：
   使用 Lua 腳本保證原子性
   if redis.call("GET", key) == value then
       return redis.call("DEL", key)
   else
       return 0
   end
```

**為什麼釋放鎖需要 Lua 腳本**：
```
錯誤做法（非原子）：
1. value = redis.GET(key)
2. if value == my_value:
3.     redis.DEL(key)

問題：
- 步驟 1 和 2 之間，鎖可能過期
- 步驟 2 和 3 之間，其他客戶端可能獲取了鎖
- 導致誤刪其他客戶端的鎖

Lua 腳本解決：
- 在 Redis 服務器端原子執行
- 中間不會被其他命令打斷
```

**優缺點分析**：
- ✅ 性能高、延遲低
- ✅ 實現簡單清晰
- ⚠️ **單點 Redis 不可靠**（主從切換時可能丟失鎖）
- ⚠️ 需要處理鎖續期問題

**結論**：適合絕大多數場景，特別是對性能要求高的場景。

#### 3.3 方案三：ZooKeeper 實現

**為什麼選擇 ZooKeeper**：
- ✅ **強一致性**：基於 ZAB 協議（類似 Raft）
- ✅ **自動容錯**：客戶端斷線，臨時節點自動刪除
- ✅ **公平鎖**：按順序排隊，先到先得

**實現原理**：
```
利用臨時順序節點：
1. 客戶端在 /locks/resource_123/ 下創建臨時順序節點
   /locks/resource_123/lock_0000000001
   /locks/resource_123/lock_0000000002
   /locks/resource_123/lock_0000000003

2. 序號最小的客戶端持有鎖

3. 其他客戶端監聽（watch）前一個節點

4. 前一個節點刪除時，下一個客戶端獲得鎖

5. 客戶端斷線，臨時節點自動刪除
```

**優缺點分析**：
- ✅ **強一致性保證**
- ✅ **自動死鎖處理**（臨時節點）
- ✅ **公平性**（FIFO 順序）
- ❌ **性能較差**：QPS ~5000-10000
- ❌ **運維複雜**：需要維護 ZooKeeper 集群
- ❌ **延遲較高**：網路往返 + 共識開銷

**結論**：適合對一致性要求極高的場景（如 leader 選舉、配置管理）。

#### 3.4 方案四：etcd 實現

**特點**：
- 類似 ZooKeeper，但更現代化
- 基於 Raft 協議
- 提供 Lease（租約）機制

**實現方式**：
```
1. 創建 Lease（租約，TTL 10 秒）
2. 在 key 上設置 Lease
3. 定期 KeepAlive 續期
4. 主動釋放或自動過期
```

**與 ZooKeeper 對比**：
- 性能略優於 ZooKeeper
- API 更友好（gRPC）
- 雲原生生態（Kubernetes 使用）

**結論**：適合雲原生環境，替代 ZooKeeper 的新選擇。

#### 3.5 方案選擇總結

| 方案 | 性能 | 一致性 | 複雜度 | 推薦場景 |
|------|------|--------|--------|---------|
| **數據庫** | ⭐ | ⭐⭐⭐ | ⭐ | 低並發 |
| **Redis** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐ | 高性能場景（推薦） |
| **ZooKeeper** | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 強一致性需求 |
| **etcd** | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 雲原生環境 |

**選擇建議**：
```
性能優先：Redis 單實例
可靠性優先：ZooKeeper / etcd
平衡方案：Redlock（Redis 多實例）
```

### 4. Redis 鎖的進階問題

#### 4.1 問題一：鎖超時問題

**場景**：
```
時間軸：
0s:  客戶端 A 獲取鎖（TTL 10s）
0s:  客戶端 A 開始執行業務
9s:  業務還在執行...
10s: 鎖自動過期釋放
10s: 客戶端 B 獲取鎖成功
12s: 客戶端 A 完成業務
12s: 客戶端 A 嘗試釋放鎖（但已被 B 持有）

結果：❌ A 和 B 同時在臨界區！
```

**解決方案一：設置更長的 TTL**
```
問題：業務執行時間不確定
風險：設置過長，死鎖時間也變長

建議：TTL = 業務時間 P99 × 2
```

**解決方案二：Watchdog 自動續期**
```
原理：
1. 獲取鎖時設置 TTL 10s
2. 啟動後台線程
3. 每 3 秒（TTL/3）檢查一次
4. 如果業務還在執行，續期 10s
5. 業務完成後停止續期

效果：
- 業務執行多久，鎖就持有多久
- 避免業務執行中鎖過期

實現庫：
- Redisson（Java）自動實現 Watchdog
- Go 需要自己實現
```

**解決方案三：Fencing Token（最可靠）**
```
原理：
1. 每次獲取鎖時，同時獲取一個遞增的 token
2. 操作資源時帶上 token
3. 資源服務檢查 token，只接受更大的 token

示例：
- 客戶端 A 獲取鎖，token = 123
- 客戶端 A 鎖過期
- 客戶端 B 獲取鎖，token = 124
- 客戶端 A 請求資源，token = 123（❌ 拒絕）
- 客戶端 B 請求資源，token = 124（✅ 接受）

優點：
- 從根本上避免了鎖過期問題
- 即使鎖失效，也不會操作錯誤

缺點：
- 需要資源服務配合
- 實現複雜度高
```

#### 4.2 問題二：Redis 單點故障

**場景**：
```
1. 客戶端 A 從 Redis Master 獲取鎖
2. Master 在鎖資料複製到 Slave 前崩潰
3. Slave 晉升為新 Master（沒有鎖資料）
4. 客戶端 B 從新 Master 獲取鎖成功
5. ❌ A 和 B 同時持有鎖
```

**解決方案：Redlock 算法**

**核心思想**：使用多個獨立的 Redis 實例（通常 5 個）。

**算法流程**：
```
1. 獲取當前時間 T1
2. 依次嘗試從 5 個 Redis 實例獲取鎖
3. 設置單個獲取超時（如 5ms）
4. 統計成功獲取的實例數
5. 如果成功 >= 3 個（N/2 + 1）且總耗時 < TTL
   - 獲取鎖成功
   - 鎖的實際有效時間 = TTL - (當前時間 - T1)
6. 否則，釋放所有已獲取的鎖
```

**為什麼需要 5 個實例**：
```
容錯能力：
- 5 個實例，需要 3 個成功
- 可容忍 2 個實例故障
- 故障容忍率：40%

為什麼不用 3 個：
- 3 個實例，需要 2 個成功
- 只能容忍 1 個故障（容忍率 33%）
- 可靠性較低
```

**Redlock 的爭議**：

著名的 Martin Kleppmann 批評 Redlock 不可靠：
```
問題 1：時鐘跳躍
- 如果某個 Redis 實例的時鐘快進
- 鎖可能提前過期
- 破壞互斥性

問題 2：GC 暫停
- 客戶端 GC 暫停 15 秒
- 鎖已過期但客戶端不知道
- 恢復後誤以為還持有鎖
```

Redis 作者 Antirez 的回應：
```
Redlock 不追求絕對的正確性
適用於效率型場景（如秒殺）
需要絕對正確性請用 ZooKeeper
```

**實踐建議**：
```
絕大多數場景：Redis 單實例 + Watchdog
高可靠性需求：ZooKeeper / etcd
折衷方案：Redlock（理解其限制）
```

### 5. 分散式鎖的最佳實踐

#### 5.1 避免常見錯誤

**錯誤一：鎖粒度過大**
```
❌ 錯誤：
lock = acquire("global_lock")
process_order(order_id)
release(lock)

問題：所有訂單處理都串行，吞吐量極低

✅ 正確：
lock = acquire(f"order_lock:{order_id}")
process_order(order_id)
release(lock)

效果：不同訂單可並行處理
```

**錯誤二：忘記設置過期時間**
```
❌ 錯誤：
redis.SET(lock_key, value, NX)  // 沒有 EX

風險：客戶端崩潰，鎖永不釋放（死鎖）

✅ 正確：
redis.SET(lock_key, value, NX, EX, 10)
```

**錯誤三：不檢查鎖持有者**
```
❌ 錯誤：
def unlock(key):
    redis.DEL(key)  // 可能刪除別人的鎖

✅ 正確：
def unlock(key, my_value):
    lua_script = """
    if redis.call("GET", KEYS[1]) == ARGV[1] then
        return redis.call("DEL", KEYS[1])
    else
        return 0
    end
    """
    redis.eval(lua_script, 1, key, my_value)
```

#### 5.2 性能優化

**優化一：減少鎖競爭**
```
問題：多個客戶端頻繁競爭同一把鎖

解決方案 1：分段鎖
- 將資源分成 16 個段
- 每個段一把鎖
- 降低單鎖競爭

解決方案 2：本地快取
- 先檢查本地快取
- 未命中才加分散式鎖
- 減少鎖使用頻率
```

**優化二：自旋等待 vs 直接失敗**
```
自旋等待（適合鎖持有時間短）：
for i in range(max_retries):
    if acquire_lock():
        return True
    time.sleep(0.01)  // 10ms 後重試
return False

直接失敗（適合鎖持有時間長）：
if acquire_lock():
    return True
else:
    return False  // 立即失敗
```

### 6. 監控與排查

#### 6.1 關鍵指標

**業務指標**：
- 鎖獲取成功率（> 80%）
- 鎖等待時間 P99（< 100ms）
- 鎖持有時間分佈
- 鎖超時次數（應接近 0）

**系統指標**：
- Redis 可用性
- 鎖相關 Redis 命令 QPS
- 鎖相關 Redis 命令延遲

#### 6.2 常見問題排查

**問題：鎖獲取成功率低**
```
可能原因：
- 鎖粒度過大（單一鎖競爭激烈）
- TTL 設置過長（鎖釋放慢）
- 業務執行時間過長

排查方法：
- 查看鎖持有時間分佈
- 檢查是否有長時間持鎖的情況
- 考慮細化鎖粒度
```

**問題：懷疑出現死鎖**
```
排查步驟：
1. 檢查 Redis 中是否有長期存在的鎖 key
2. 查看鎖的 TTL（PTTL lock_key）
3. 如果 TTL = -1，說明沒有設置過期時間（bug）
4. 如果 TTL > 預期值，可能是 Watchdog 異常

應急處理：
- 手動刪除鎖 key：DEL lock_key
- 但要確認確實是死鎖，不是正在使用
```

## 程式碼範例（可選）

僅展示 Redis 鎖的核心實現：

```go
// 獲取鎖
func AcquireLock(key string, value string, ttl time.Duration) bool {
    result, _ := redis.SetNX(key, value, ttl).Result()
    return result
}

// 釋放鎖（Lua 腳本保證原子性）
func ReleaseLock(key string, value string) bool {
    script := `
    if redis.call("GET", KEYS[1]) == ARGV[1] then
        return redis.call("DEL", KEYS[1])
    else
        return 0
    end
    `
    result, _ := redis.Eval(script, []string{key}, value).Int()
    return result == 1
}
```

## 總結

設計分散式鎖的核心要點：

1. **方案選擇**：Redis（性能優先） vs ZooKeeper（一致性優先）
2. **避免死鎖**：設置 TTL 自動過期
3. **防止誤刪**：鎖帶唯一標識，釋放時檢查（Lua 腳本）
4. **處理超時**：Watchdog 續期 或 Fencing Token
5. **高可用**：Redlock 多實例 或 ZooKeeper 集群
6. **性能優化**：細化鎖粒度、減少競爭、本地快取

分散式鎖的本質是**在分散式環境下實現互斥訪問**。沒有完美的方案，只有針對場景的最優選擇。理解不同方案的權衡（CAP 理論），是設計分散式鎖的關鍵。
