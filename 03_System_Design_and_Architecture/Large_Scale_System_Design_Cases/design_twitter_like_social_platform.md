# 如何設計類 Twitter 的社交平台？

- **難度**: 9
- **重要程度**: 5
- **標籤**: `System Design`, `Social Platform`, `Feed Architecture`, `Fanout`

## 問題詳述

設計一個類似 Twitter 的社交媒體平台，支援使用者發佈推文（Tweet）、關注其他使用者、查看時間線動態（Timeline Feed）等核心功能。系統需要處理數億使用者和每秒數十萬次請求。

## 核心理論與詳解

### 1. 需求澄清與系統範圍

在系統設計面試中，首先需要明確需求範圍，避免過於發散。

#### 1.1 核心功能

- **發佈推文**：使用者可以發佈最多 280 字元的短文
- **關注系統**：使用者可以關注/取消關注其他使用者
- **時間線查詢**：查看關注使用者的推文動態（Home Timeline）
- **個人頁面**：查看特定使用者的所有推文（User Timeline）
- **互動功能**：點讚、轉推、回覆

#### 1.2 非功能性需求

- **高可用性**：99.99% 可用性，服務不能中斷
- **讀寫比例**：典型的讀多寫少場景（約 100:1）
- **延遲要求**：Feed 載入 < 200ms，發文 < 500ms
- **一致性權衡**：發文需要強一致性，Feed 可接受最終一致性

#### 1.3 容量估算

**流量估算**：
```
假設：
- 日活使用者（DAU）：200M
- 每使用者每日發文：2 次
- 每使用者每日查看 Feed：50 次
- 平均關注數：200 人

計算：
- 發文 QPS：200M × 2 / 86400 ≈ 4,600 QPS（峰值 15,000）
- Feed 查詢 QPS：200M × 50 / 86400 ≈ 115,000 QPS（峰值 350,000）
```

**儲存估算**：
```
- 單條推文：~500 bytes（含 metadata）
- 每日新增：400M tweets × 500 bytes ≈ 200 GB/day
- 5 年總量：~365 TB（含備份和索引）
```

### 2. 核心架構設計

#### 2.1 高層架構概覽

```
┌─────────────┐
│   Client    │
└──────┬──────┘
       │
┌──────▼──────────────────────────────────┐
│        Load Balancer (CDN)              │
└──────┬──────────────────────────────────┘
       │
┌──────▼──────────────────────────────────┐
│        API Gateway                      │
│  - Rate Limiting                        │
│  - Authentication                       │
└──────┬──────────────────────────────────┘
       │
       ├────────────────┬────────────────┐
       │                │                │
┌──────▼──────┐  ┌─────▼──────┐  ┌─────▼──────┐
│   Tweet     │  │   Feed     │  │   User     │
│  Service    │  │  Service   │  │  Service   │
└──────┬──────┘  └─────┬──────┘  └─────┬──────┘
       │                │                │
┌──────▼────────────────▼────────────────▼──────┐
│           Data Storage Layer                  │
│  - MySQL (推文、使用者)                        │
│  - Redis (快取、Feed)                         │
│  - Cassandra (關注關係圖)                     │
└───────────────────────────────────────────────┘
```

#### 2.2 資料模型設計

**推文表（Tweets）**：
- 儲存選擇：MySQL 或 Cassandra
- 原因：推文一旦發佈不常修改，適合 NoSQL 的 append-only 特性
- 分片策略：按 `tweet_id` 或 `user_id` 分片

**使用者表（Users）**：
- 儲存選擇：MySQL
- 原因：使用者資料需要事務支持（如修改個人資訊）

**關注關係圖（Social Graph）**：
- 儲存選擇：Redis 或 Cassandra
- 資料結構：
  - `followers:{user_id}` → Set of follower IDs
  - `following:{user_id}` → Set of following IDs
- 原因：需要快速查詢「誰關注了我」和「我關注了誰」

### 3. Feed 生成策略（系統設計核心）

這是 Twitter 類系統最關鍵的設計決策。有三種主要方案，各有優劣：

#### 3.1 方案一：Fanout-on-Write（寫擴散）

**原理**：使用者發文時，立即將推文推送到所有粉絲的 Feed 中。

**流程**：
```
1. 使用者 A 發佈推文
2. 查詢 A 的所有粉絲列表（1000 個粉絲）
3. 將推文 ID 寫入每個粉絲的 Feed 快取
   - Redis: `feed:{follower_1}` → [tweet_id_1, tweet_id_2, ...]
   - Redis: `feed:{follower_2}` → [tweet_id_1, tweet_id_2, ...]
   - ...
4. 使用者查看 Feed 時，直接從快取讀取
```

**優點**：
- ✅ **讀取極快**：Feed 已預先生成，直接從 Redis 讀取（< 10ms）
- ✅ **架構簡單**：讀取邏輯非常直接

**缺點**：
- ❌ **寫入壓力大**：發文需要寫入 N 個粉絲的 Feed（N = 粉絲數）
- ❌ **明星使用者問題**：擁有百萬粉絲的使用者發文，需要寫入百萬次
- ❌ **儲存成本高**：每個使用者的 Feed 都需要獨立儲存

**適用場景**：粉絲數較少的普通使用者（< 10,000 粉絲）

#### 3.2 方案二：Fanout-on-Read（讀擴散）

**原理**：使用者查看 Feed 時，即時從關注的所有人那裡拉取最新推文。

**流程**：
```
1. 使用者 B 查看 Feed
2. 查詢 B 關注的使用者列表（200 人）
3. 查詢這 200 人的最新推文
4. 合併排序，返回 Top 100
```

**優點**：
- ✅ **寫入簡單**：發文只需寫入一次（自己的 Timeline）
- ✅ **沒有明星使用者問題**：不管粉絲多少，寫入成本都是 O(1)
- ✅ **儲存成本低**：不需要為每個使用者維護 Feed 快取

**缺點**：
- ❌ **讀取延遲高**：需要查詢多個使用者的 Timeline 並排序
- ❌ **查詢壓力大**：每次 Feed 請求都需要多次資料庫查詢
- ❌ **難以優化**：即使有快取，複雜度仍然較高

**適用場景**：關注數較少的場景，或對延遲不敏感的系統

#### 3.3 方案三：Hybrid（混合方案，推薦）

**原理**：根據使用者類型採用不同策略，結合兩種方案的優點。

**策略劃分**：
- **普通使用者**（< 10,000 粉絲）：使用 Fanout-on-Write
- **明星使用者**（> 10,000 粉絲）：使用 Fanout-on-Read
- **Feed 生成**：合併兩種策略的結果

**流程**：
```
使用者 C 查看 Feed：
1. 從 Redis 讀取預生成的 Feed（Fanout-on-Write 的結果）
2. 查詢 C 關注的明星使用者列表
3. 即時拉取這些明星使用者的最新推文
4. 合併排序兩部分資料
5. 返回 Top 100
```

**優點**：
- ✅ **平衡讀寫壓力**：大部分寫入成本可控，讀取性能也不錯
- ✅ **可擴展性好**：可根據系統負載動態調整閾值
- ✅ **成本可控**：避免了明星使用者的寫擴散問題

**缺點**：
- ⚠️ **系統複雜度增加**：需要維護兩套邏輯
- ⚠️ **需要明星使用者識別機制**

**為什麼 Twitter 選擇混合方案**：
Twitter 的實際場景中，明星使用者（Lady Gaga、Elon Musk 等）擁有數千萬粉絲，單純的 Fanout-on-Write 會導致每次發文需要數千萬次寫入，這是不可接受的。混合方案在實踐中被證明是最佳選擇。

### 4. 關鍵技術細節

#### 4.1 推文 ID 生成

**需求**：全域唯一、趨勢遞增、高效能生成。

**方案選擇：Snowflake 算法**

```
64-bit ID 結構：
- 1 bit: 符號位（保留）
- 41 bits: 時間戳（毫秒）
- 10 bits: 機器 ID
- 12 bits: 序列號

優點：
- 全域唯一
- 按時間排序（對 Timeline 查詢友好）
- 生成效率高（百萬/秒）
- 可反向解析時間
```

**為什麼不用 UUID**：
- UUID 是隨機的，不利於資料庫索引和排序
- UUID 是 128-bit，佔用空間更大

#### 4.2 快取策略

**多層快取架構**：

```
L1: CDN
- 靜態資源（圖片、影片）
- 熱門推文頁面

L2: Redis（應用層快取）
- 使用者 Feed：`feed:{user_id}` → List of tweet_ids（最新 1000 條）
- 推文內容：`tweet:{tweet_id}` → Tweet object
- 使用者資訊：`user:{user_id}` → User object

L3: 資料庫查詢結果快取
- Feed 未命中時的兜底查詢
```

**快取淘汰策略**：
- Feed 快取：保留最新 1000 條推文 ID
- 推文內容快取：LRU 淘汰，TTL 24 小時
- 熱點推文：額外延長 TTL

**快取一致性**：
- 發文時：寫資料庫 + 寫快取（Cache-Aside）
- 修改推文：刪除快取（Cache-Invalidation）

#### 4.3 資料庫分片策略

**推文表分片**：
- 分片鍵：`user_id`
- 原因：查詢 User Timeline 時可以定位到單個分片
- 分片數：256 個（根據容量估算）

**關注關係圖分片**：
- 分片鍵：`follower_id`（查詢「我關注了誰」）或 `followee_id`（查詢「誰關注了我」）
- 需要兩份索引以支持雙向查詢

### 5. 系統擴展性與優化

#### 5.1 水平擴展

**無狀態服務層**：
- 所有應用伺服器都是無狀態的，可隨意增減
- 通過 Load Balancer 分發流量

**資料層擴展**：
- MySQL：主從複製 + 讀寫分離
- Redis：Redis Cluster（16 個 master 節點）
- Cassandra：天然支持水平擴展

#### 5.2 效能優化

**非同步處理**：
- 發文後的 Fanout 操作放入訊息佇列（Kafka）
- Feed 預熱任務非同步執行
- 通知推送非同步處理

**批次操作**：
- Fanout 時批次寫入多個使用者的 Feed
- 使用 Redis Pipeline 減少網路往返

**預載和預熱**：
- 熱門使用者的 Timeline 常駐快取
- 登入時預載使用者 Feed 的前 100 條

### 6. 高可用性設計

#### 6.1 故障容錯

**服務降級**：
- Redis 故障時，降級到資料庫查詢（慢但可用）
- 推薦演算法失敗時，返回基礎時間線

**熔斷機制**：
- 單個服務異常時，觸發熔斷避免雪崩
- 超時保護：設定合理的超時時間（200ms）

#### 6.2 資料一致性

**最終一致性接受度**：
- Feed 延遲幾秒是可接受的（最終一致性）
- 發文本身必須強一致性（寫入成功才返回）

**資料複製**：
- 多資料中心部署
- 跨區域資料複製（異步）

### 7. 常見面試追問

#### Q1：如何處理熱點推文？

**問題**：某條推文突然爆紅，短時間內百萬次讀取。

**解決方案**：
- **識別熱點**：通過訪問統計識別熱門推文
- **額外快取**：將熱點推文放入 CDN 和本地快取
- **限流保護**：對單條推文的訪問進行限流
- **預計算**：提前生成推文的渲染結果

#### Q2：如何防止系統被濫用（發垃圾推文）？

**解決方案**：
- **限流**：每使用者每小時最多 100 條推文
- **驗證碼**：檢測異常行為時要求驗證碼
- **內容審核**：關鍵字過濾 + 機器學習識別
- **帳號信譽系統**：新帳號限制更嚴格

#### Q3：如何實現 @mention 功能？

**設計**：
- **提取 @username**：發文時解析文本提取被提及的使用者
- **通知機制**：發送通知給被提及的使用者
- **關聯儲存**：在推文表中記錄 mentioned_user_ids
- **查詢優化**：建立索引支持「查詢提到我的推文」

## 程式碼範例（可選）

僅展示核心概念的簡化實現：

```go
// Feed 生成（混合方案）
func GetUserFeed(userID int64, limit int) []Tweet {
    // 1. 從 Redis 獲取預生成的 Feed（Fanout-on-Write）
    cachedFeedIDs := redis.LRange(fmt.Sprintf("feed:%d", userID), 0, limit)
    
    // 2. 查詢關注的明星使用者
    celebrityIDs := getCelebrityFollowings(userID)
    
    // 3. 即時拉取明星使用者的推文（Fanout-on-Read）
    celebrityTweets := fetchRecentTweets(celebrityIDs, limit)
    
    // 4. 合併並排序
    allTweets := merge(cachedFeedIDs, celebrityTweets)
    sort.Slice(allTweets, func(i, j int) bool {
        return allTweets[i].Timestamp > allTweets[j].Timestamp
    })
    
    return allTweets[:limit]
}
```

## 總結

設計 Twitter 類社交平台的核心挑戰在於：

1. **Feed 生成策略選擇**：混合 Fanout-on-Write 和 Fanout-on-Read 以平衡讀寫壓力
2. **處理明星使用者**：避免寫擴散導致的百萬級寫入
3. **快取架構**：多層快取降低資料庫壓力
4. **可擴展性**：無狀態服務 + 資料分片支持水平擴展
5. **一致性權衡**：發文強一致，Feed 最終一致


這個系統體現了分散式系統設計中的經典權衡：**一致性 vs 可用性**、**讀性能 vs 寫性能**、**儲存成本 vs 計算成本**。理解這些權衡是系統設計的關鍵。
